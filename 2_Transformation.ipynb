{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "698f8466-862a-4295-9206-40e15a67483d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a470b2e-6463-4b1c-a257-e23d0b801ca6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TextBlob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nltk>=3.8\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/siddharthsingh/anaconda3/envs/env1/lib/python3.11/site-packages (from nltk>=3.8->TextBlob) (1.2.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk, TextBlob\n",
      "Successfully installed TextBlob-0.18.0.post0 click-8.1.7 nltk-3.8.1 regex-2023.12.25 tqdm-4.66.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2520161-8ade-456f-a4b6-e1f66c8c2bc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'databricks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhashlib\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatabricks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_store\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatabricks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_store\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     25\u001b[0m filterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'databricks'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from warnings import filterwarnings\n",
    "import math\n",
    "import hashlib\n",
    "from databricks import feature_store\n",
    "from databricks.feature_store import *\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb320333-ef44-4547-9dbd-ce6502c30c42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Dowload nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6b5d8ad-e758-4ed7-b020-6ee6b6fc0ce8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135b7b04-b163-4f3d-b7de-dcd37cdcbb9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load Customer Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b01f452-c30f-43d5-aed5-c216b13ddd9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "data = spark.sql(\"select * from projects.customer_review_dataset\").toPandas()\n",
    "\n",
    "# replacement dictionary generated from EDA\n",
    "with open('replace_dict.pkl', 'rb') as f:\n",
    "  replacement_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc31a51b-7c30-4cee-ba61-f0e11bbbc5cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5508ca9-4416-4da9-be87-846cc5708b1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def replace_shortwords(replacement_dict,text):\n",
    "  \"\"\"function to replace short words to proper words using a replacement dictionary eg. awsm to awesome\n",
    "  replacement_dict : dict : dictionary created in EDA process\n",
    "  text : string : text which need to be treated\"\"\"\n",
    "  word_list= word_tokenize(text)\n",
    "  for r in replacement_dict.keys():\n",
    "      word_list=list(pd.Series(word_list).replace(r,replacement_dict[r]))\n",
    "  return \" \".join(word_list)\n",
    "\n",
    "def generate_hash_key(row):\n",
    "  \"\"\"function to generate unique hash key from columns in dataframe\n",
    "  row : pd.Series : row in pandas dataframe\"\"\"\n",
    "  columns = [row.Rate, row.Sentiment, row.combined_cleaned,row.combined_cleaned_lemmatized,row.index] # replace with the actual column names\n",
    "  concatenated_data = ''.join(str(column) for column in columns)\n",
    "  hash_key = hashlib.md5(concatenated_data.encode()).hexdigest()\n",
    "  return hash_key\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "def ReviewProcessing(df):\n",
    "  \"\"\"function to remove stopwords\n",
    "  df : pd.DataFrame : Dataframe which contains 'combined_cleaned' column\"\"\"\n",
    "  # remove non alphanumeric\n",
    "  df['combined_cleaned'] = df.combined_cleaned.str.replace('[^a-zA-Z0-9 ]', '')\n",
    "  # lowercase\n",
    "  df.combined_cleaned = df.combined_cleaned.str.lower()\n",
    "  # split into list\n",
    "  df.combined_cleaned = df.combined_cleaned.str.split(' ')\n",
    "  # remove stopwords\n",
    "  df.combined_cleaned = df.combined_cleaned.apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "  return df\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "  \"\"\"function to perform Lemmatization on text\n",
    "  word : string : word in a paragraph\"\"\"\n",
    "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "  tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "  return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def get_lemmatize(sent):\n",
    "  \"\"\"function to perfrom Lemmatization on text\n",
    "  sent : string : sentence\"\"\"\n",
    "  return \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sent)])\n",
    "\n",
    "def polarity(text):\n",
    "  \"\"\"function to calculate polarity score using TextBlob from a paragraph\"\"\"\n",
    "  return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bbd8907-7fdb-4d63-8f9a-b0ee64ddb538",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ab89dd-05fc-4155-8215-fcc68710e2e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Filtering and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d80faa6-6d41-415c-bb59-bcff01dab826",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Removing Null Records from Summary\n",
    "data=data.dropna(subset='Summary')\n",
    "\n",
    "# Filling Null records in Review with blank\n",
    "data['Review']=data['Review'].fillna('')\n",
    "\n",
    "# Removing discrepencies in Rate column\n",
    "try:\n",
    "    data = data[~data['Rate'].isin(['Pigeon Favourite Electric Kettle??????(1.5 L, Silver, Black)',\n",
    "        'Bajaj DX 2 L/W Dry Iron',\n",
    "        'Nova Plus Amaze NI 10 1100 W Dry Iron?Ã\\x83Â¿?Ã\\x83Â¿(Grey & Turquoise)'])]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Convert Rate to numeric\n",
    "data['Rate'] = pd.to_numeric(data['Rate'])\n",
    "\n",
    "# Since we are doing sentiment classification, We don't need Product Price, so let's drop it\n",
    "data.drop(['product_price','product_name'],axis=1,inplace=True)\n",
    "\n",
    "# Replacing Null records in Review with blank\n",
    "data['Review']=data['Review'].fillna('')\n",
    "data['combined']=data['Review']+' '+data['Summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6d7faf-33e0-4369-9c68-1a47be861b3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Replacing short words to proper words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa136c5-9136-4216-8e3e-90d10dea182d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replacing short words with proper words\n",
    "data=data.reset_index(drop=True)\n",
    "result = Parallel(verbose = 0, n_jobs=multiprocessing.cpu_count())(delayed(replace_shortwords)(replacement_dict=replacement_dict,text=x) for x in tqdm(data['combined']))\n",
    "data['combined_cleaned']=pd.Series(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1041fe7-afa1-4b27-851a-a623537373ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Polarity and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adce16d6-8597-40c7-a27d-8a500d8cf0f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can calculate polarity from combined text now and filter those records which doesn't make sense i.e eg. sentiment is negative but polarity is >0.8\n",
    "data['polarity_score_combined']=data['combined_cleaned'].apply(lambda x : polarity(x))\n",
    "\n",
    "# Remove these rows because these are adding noise\n",
    "data = data[~((data['Sentiment']=='positive')&(data['polarity_score_combined']<0))]\n",
    "\n",
    "# Remove these rows because these are adding noise\n",
    "data = data[~((data['Sentiment']=='negative')&(data['polarity_score_combined']>0.3))]\n",
    "\n",
    "# Remove these rows because these are adding noise\n",
    "data=data[~((data['Sentiment']=='neutral')&(data['polarity_score_combined']>0.7))]\n",
    "\n",
    "# Remove these rows because these are adding noise\n",
    "data=data[~((data['Sentiment']=='neutral')&(data['polarity_score_combined']<-0.5))]\n",
    "\n",
    "# Remove these records, these are adding noise\n",
    "data=data[~((data['Sentiment']=='positive')&(data['Rate']<3))]\n",
    "\n",
    "# Remove these records, these are adding noise\n",
    "data=data[~((data['Sentiment']=='negative')&(data['Rate']>3))]\n",
    "\n",
    "# Remove these records, these are adding noise \n",
    "data=data[~((data['Sentiment']=='neutral')&((data['Rate']>4)|(data['Rate']<2)))]\n",
    "\n",
    "# Drop columns which are not required\n",
    "data.drop(['Review','Summary','combined','polarity_score_combined'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91547321-989c-46ee-98e9-302b3b48c5c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b773611-642d-47d5-929e-193722143c52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clean_data = ReviewProcessing(data)\n",
    "clean_data.combined_cleaned = clean_data.combined_cleaned.apply(' '.join)\n",
    "clean_data['combined_cleaned_lemmatized'] = clean_data.combined_cleaned.apply(get_lemmatize)\n",
    "clean_data=clean_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0721bba0-dc05-4964-a0bd-6780be3c94b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create unique 'id' - hashkey for feature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f92f682b-b414-4702-8b70-1f348ee5026c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clean_data['id']=clean_data.apply(lambda x:generate_hash_key(x),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57673398-b7ea-4145-b445-f4b2a4ae6415",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Feature Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fb27d76-5d5e-4d91-a89f-2f75b5c64db6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Feature Table\n",
    "fs = FeatureStoreClient()\n",
    "spark.sql(\"Create database if not exists feature_store\")\n",
    "feature_table_name=\"feature_store.customer_sentiment_analysis_01\"\n",
    "spark_df=spark.createDataFrame(clean_data)\n",
    "try:\n",
    "    # creating new feature table everytime\n",
    "    fs.drop_table(feature_table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "fs.create_table(\n",
    "    name=feature_table_name,\n",
    "    primary_keys=['id'],\n",
    "    df=spark_df,\n",
    "    description=\"Customer Sentiment Analysis Transformed Dataset\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1910131441362965,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2_Transformation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
